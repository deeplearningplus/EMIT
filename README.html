<h1>EMIT: End-Motif Inspection via Transformer.</h1>
<h2>Introduction</h2>
<p>End-motif of plasma cell-free DNA (cfDNA) is a fragmentomic marker for cancer diagnosis. Here, we presented a self-supervised learning approach – end-motif inspection via transformer (EMIT) – that learns feature represenations of cfDNA end-motifs. We demonstrated that high classification performance in the identification of cancer via linear projection of features extracted from pretrained EMIT.</p>
<h2>System requirements</h2>
<ul>
<li>Operating systems: CentOS 7.</li>
<li><a href="https://docs.conda.io/en/latest/miniconda.html">Python</a> (version == 3.7).</li>
<li><a href="https://pytorch.org">PyTorch</a> (version == 1.13.1+cu116).</li>
<li><a href="https://huggingface.co/docs/transformers/index">transformers</a> (version == 4.28.1).</li>
</ul>
<p>This example was tested with the following environment. However, it should work on the other platforms.</p>
<h2>Installation guide</h2>
<ul>
<li>Following instruction from <a href="https://docs.conda.io/en/latest/miniconda.html">miniconda</a> to install Python.</li>
<li>Use the following command to install required packages.</li>
</ul>
<pre><code class="language-bash"># Install with GPU support. Check https://pytorch.org for more information. 
#+The following cmd install PyTorch compiled with cuda 118. 
pip install torch --index-url https://download.pytorch.org/whl/cu118

# If GPU not available, install the PyTorch compiled for CPU.
pip install torch --index-url https://download.pytorch.org/whl/cpu

# Install transformers, tokenizers and prettytable
pip install transformers==4.28.1 tokenizers==0.13.3 prettytable
</code></pre>
<ul>
<li>The installation process will take about an hour. This heavily depends on your network bandwidth.</li>
</ul>
<h2>Demo</h2>
<ul>
<li>Clone <code>EMIT</code> locally from Github.</li>
</ul>
<pre><code class="language-bash">git clone https://github.com/deeplearningplus/EMIT.git
</code></pre>
<ul>
<li>Instructions to run on data:</li>
</ul>
<pre><code class="language-bash"># Run on GPU
bash pretrain_gpu.sh

# Run on CPU
bash pretrain_cpu.sh
</code></pre>
<p>The pretrained model will be saved in <code>model-example</code> when the above command finishes running.
We uploaded a pretrained model in <code>model</code> for this tutorial.</p>
<ul>
<li>Linear projection from the pretrained model</li>
</ul>
<pre><code class="language-python"># Run on GPU
bash pretrain_gpu.sh

# Run on CPU
bash pretrain_cpu.sh
</code></pre>
<p>The outputs include log file <code>log.txt</code>, checkpoint of the linear classification at each epoch and prediction probabilities on the testing set.</p>
<h2>How to run on your own data</h2>
<ul>
<li>Pretraining stage: prepare the pretraining data in the same format as <code>data/pretrained.trn.txt.gz</code> and run <code>pretrain.sh</code>.</li>
<li>Linear projection stage: prepare the data in the same format as <code>data/targeted_BS_HCC_train_fold0.csv.gz</code> and run <code>linear_probe.sh</code>.</li>
</ul>
